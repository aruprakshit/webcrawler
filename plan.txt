╭──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│  > Suppose I was asked in a system design interview I have to design WebCrawler. The problem statement was  Given a set of seed URLs, crawl all the web pages, download them. Extract the URLs amd add them to   │
│    the list and continue. Should scale out. Now read the my system deisgn template @template.txt and write how to solve this problem following the template. Also use ASCII text block for any diagrams.         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯

✦ Of course. Here is a system design for a web crawler based on the provided template.

  (1) FEATURE EXPECTATIONS

   * (1) Use cases:
       * The system will start with a set of "seed" URLs.
       * It will crawl these URLs, download the web page content (HTML).
       * It will parse the downloaded HTML to extract new URLs.
       * These new URLs will be added to a queue to be crawled.
       * The process continues indefinitely.
       * The system must be highly scalable to crawl billions of pages.
       * It must handle duplicate URLs gracefully (crawl a URL only once).
       * It must be "polite" by respecting robots.txt files and not overwhelming any single web server.

   * (2) Scenarios that will not be covered:
       * Executing JavaScript to render dynamic web pages. We will only parse the initial HTML response.
       * Crawling non-HTML content like images, videos, PDFs, etc.
       * Real-time crawling; the system will operate as a continuous, large-scale batch process.
       * Advanced handling of crawler traps (e.g., infinite calendar links).

   * (3) Who will use: This is a backend system. The primary consumer would be an internal service, such as a search engine's indexing system.

   * (4) How many will use: A small number of internal services. The load is generated by the crawler itself, not by end-users.

   * (5) Usage patterns: The system will be running 24/7, continuously fetching URLs from a frontier, downloading pages, and adding new URLs back to the frontier.

  (2) ESTIMATIONS

  Let's assume a target of crawling 1 billion pages in a 30-day period.

   * (1) Throughput:
       * 1,000,000,000 pages / (30 days  24 hours/day  3600 sec/hour) ≈ 385 pages/sec.

   * (2) Latency expected from the system:
       * Latency for any single page is not a primary goal, as this is not a user-facing system. The overall throughput is the key metric.

   * (3) Read/Write ratio:
       * This is a write-heavy system. Writes include adding new URLs to the queue and storing downloaded content. Reads include fetching URLs to be crawled and checking if a URL has been seen before.

   * (4) Traffic estimates:
       * Write:
           * QPS: ~385 pages/sec.
           * Volume: Assuming an average page size of 100 KB, data ingestion is 385  100 KB/sec = 38.5 MB/s*.
       * Read:
           * QPS: ~385 URLs/sec are read from the queue.
           * If each page has an average of 10 new links, we'll perform 385  10 = ~3850 checks/sec* against our "URL seen" data store.

   * (5) Storage estimates:
       * Content Storage: 1 billion pages  100 KB/page = 100 TB* over 30 days.
       * URL Storage: 1 billion URLs  ~200 bytes/URL (for URL, status, timestamp) = 200 GB*.

   * (6) Memory estimates:
       * Cache: We need a very fast, in-memory way to check if a URL has been seen before to avoid hitting our main database for every link. A Bloom Filter is an excellent choice for this. For 1 billion URLs with a
         1% false-positive rate, it would require about 1.2 GB of memory.
       * We should also cache robots.txt files. If we crawl 10 million unique domains, with an average file size of 2 KB, this would be 10M  2KB = 20 GB*. A distributed cache like Redis would be suitable.

  (3) DESIGN GOALS

   * (1) Latency and Throughput requirements: High throughput is the primary goal (~400 QPS). Low latency is a secondary concern.
   * (2) Consistency vs Availability: High Availability is critical. The crawler must be resilient to failures and continue operating. Eventual Consistency is acceptable. It's okay if the status of a URL takes a
     moment to propagate through the system.

  (4) HIGH LEVEL DESIGN

   * (1) Basic algorithm:
       1. Start with seed URLs in a URL Frontier.
       2. A pool of Crawler Workers requests URLs from the Frontier.
       3. For each URL, the worker checks robots.txt rules.
       4. If allowed, the worker downloads the page content.
       5. The content is sent to a Content Store.
       6. The content is also sent to a Parser, which extracts new links.
       7. Each new link is normalized (e.g., converting relative to absolute URLs).
       8. The normalized URL is checked against a "URL seen" filter. If new, it's submitted back to the URL Frontier.

   * (2) High level design diagram:

    1                                   +-----------------+
    2                                   |   Seed URLs     |
    3                                   +-----------------+
    4                                           |
    5                                           v
    6 +--------------------------------------------------------------------------------------+
    7 |                                    URL Frontier                                      |
    8 |                             (Manages URLs to be crawled)                             |
    9 +--------------------------------------------------------------------------------------+
   10       ^                                   |
   11       | (New & Unique URLs)               | (URLs to Crawl)
   12       |                                   v
   13 +------------------+              +------------------+
   14 | URL Seen Filter  |              |  Crawler Workers |
   15 | (Bloom Filter)   |              +------------------+
   16 +------------------+                      |
   17       ^                                   | 1. Download Page
   18       |                                   v
   19 +------------------+              +------------------+
   20 |      Parser      | <----------  |    Internet      |
   21 | (Extracts Links) |   (HTML)     +------------------+
   22 +------------------+
   23       ^
   24       |
   25 +------------------+
   26 |  Content Store   |
   27 | (S3, HDFS, etc.) |
   28 +------------------+

  (5) DEEP DIVE

   * (1) Scaling the algorithm: The main bottlenecks are the URL Frontier and the act of downloading pages. We can scale horizontally by running many stateless Crawler Workers. The URL Frontier must be designed as
     a distributed system.

   * (2) Scaling individual components:
       * URL Frontier: A simple queue is not enough. We need to enforce politeness (not hitting one domain too often) and prioritize URLs. A great solution is to use a distributed message queue like Apache Kafka.
         We can create a topic for URLs to be crawled and partition it by hostname. This ensures all URLs for a given host go to the same partition, which can be processed by a single consumer at a time, allowing
         us to enforce rate limits per domain.
       * Crawler Workers: These are stateless and can be containerized (e.g., Docker) and managed by an orchestrator like Kubernetes. We can easily scale the number of workers up or down based on the queue size in
         Kafka.
       * URL Seen Filter: The 1.2 GB Bloom filter can be replicated on each Crawler Worker for speed. The source of truth would be a distributed NoSQL database (like Cassandra or DynamoDB) which is optimized for
         high-write throughput. The Bloom filter drastically reduces the read load on this database.
       * Content Store: A cloud-based object store like AWS S3 or Google Cloud Storage is perfect. They are managed, highly durable, and scale limitlessly for our purposes.
       * DB (NoSQL): A Wide-Column NoSQL database like Cassandra is ideal for storing URL metadata. We can use a hash of the normalized URL as the primary key, which provides excellent distribution of data across
         the cluster and fast key-based lookups.
       * Caches: A distributed cache like Redis should be used to store robots.txt rules, keyed by domain name. This avoids millions of redundant downloads of the same robots.txt file.
       * Asynchronism: The entire architecture is asynchronous, with Kafka acting as the central buffer. This decouples the link extraction process (producers) from the page downloading process (consumers) and
         provides resilience and back-pressure handling.

   * (3) Detailed Architecture Diagram:

    1 +-----------+
    2 | Seed URLs |
    3 +-----------+
    4      |
    5      v
    6 +---------------------------------------------------------------------------------+
    7 |                                 Kafka (URL Frontier)                            |
    8 |                      [Topic: urls-to-crawl, Partitioned by Hostname]            |
    9 +---------------------------------------------------------------------------------+
   10      |
   11      | (Distributed Consumers)
   12      v
   13 +---------------------------------------------------------------------------------+
   14 |                            Crawler Service (Auto-scaling Fleet)                 |
   15 |                                                                                 |
   16 |  +----------+      +----------+      +----------+      +----------+             |
   17 |  | Worker 1 |      | Worker 2 |      | Worker 3 | .... | Worker N |             |
   18 |  +----------+      +----------+      +----------+      +----------+             |
   19 |       | 1. Consume URL from Kafka Partition                                     |
   20 |       | 2. Check Redis Cache for robots.txt rules                             |
   21 |       | 3. Download Page from Internet (respecting politeness delay)            |
   22 |       | 4. Put content in S3 Object Store                                       |
   23 |       | 5. Update URL status in Cassandra DB                                    |
   24 |       | 6. Parse HTML, extract new links                                        |
   25 |       | 7. For each new link:                                                   |
   26 |       |    a. Normalize it                                                      |
   27 |       |    b. Check local Bloom Filter                                          |
   28 |       |    c. If not in filter, check Cassandra (to be sure)                    |
   29 |       |    d. If truly new, produce back to Kafka                               |
   30 |       +-------------------------------------------------------------------------+
   31 |                                         |
   32 +-----------------------------------------+
   33                                           | (Produce new, unique URLs)
   34                                           v
   35                                      (To Kafka)

